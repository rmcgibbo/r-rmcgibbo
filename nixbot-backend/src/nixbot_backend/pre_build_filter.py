from __future__ import annotations

import json
import os
import random
import sys
import tempfile
import time
from argparse import ArgumentParser
from collections import Counter
from datetime import timedelta
from fnmatch import fnmatch
from typing import Counter as Counter_t
from typing import List, Optional, Tuple

import networkx as nx
from humanize import naturaldelta
from loguru import logger as log
from nixbot_common import configure_logging, isint
from precedenceConstrainedKnapsack import precedenceConstrainedKnapsack

from .github import GithubClient, determine_modified_files
from .graphtheory import dag_longest_paths
from .nix import Attr, build_dry, get_build_graph, get_estimated_build_times

MAX_BUILD_TIME = timedelta(hours=1.5)
MAX_SINGLE_ATTR_BUILD_TIME = timedelta(hours=1)
DEBUG = False

BIG_AUTOGENERATED_FILES = {
    "pkgs/development/haskell-modules/hackage-packages.nix",
    "pkgs/development/node-packages/node-packages.nix",
    "pkgs/development/r-modules/cran-packages.nix",
    "pkgs/tools/typesetting/tex/texlive/pkgs.nix",
    "pkgs/development/compilers/elm/packages/node-packages.nix",
    "pkgs/applications/version-management/gitlab/yarnPkgs.nix",
    "pkgs/servers/nosql/influxdb2/influx-ui-yarndeps.nix",
    "pkgs/build-support/rust/crates-io.nix",
    "pkgs/misc/vim-plugins/generated.nix",
    "pkgs/tools/package-management/cargo-download/crates-io.nix",
    "pkgs/servers/x11/xorg/default.nix",
}

SKIP_DRV_NAMES = [
    # Slow packages, from Sandro
    "bareos",
    "digikam",
    "iosevka",
    "librealsense",
    "libreoffice",
    "lumo",
    "pcl",
    "mrtrix",
    "simpleitk",
    "smesh",
    "torchgpipe",
    "torchvision",
    "tts",
    "ceph",
    "edward",
    "freecad",
    "opencascade",
    "sage",
    "pyro-ppl",
    "pytorch",
    "tensorflow",
    "tflearn",
    "qgis",
    "vtk",
    "wine",
    "wine-staging",
    "cudatoolkit",
    # Requires nix-prefecth-url with human intervention
    "factorio",
    "cov-analysis"
    "oraclejdk"
    "factorio-experimental",
    "plex",
    "displaylink",
    "keeperrl",
    "vessel",
    "koboredux",
    "worldofgoo",
    "andyetitmoves",
    "dxx-rebirth",
    "coursier",
    "sqldeveloper",
    "sqldeveloper_18",
    "kdbplus",
    "electrum",
    "citrix-workspace",
]

SKIP_ATTR_NAMES: List[str] = [
    # "haskellPackages.*"
    "oraclejdk",
    "cov-build",
    "sm64ex",
    "worldofoo",
    "vessel",
    "koboredux",
    "lattice-diamond",
    "ndi",
    "kdbplus",
    "houdini",
    "cplex",
    "mathematica*",
    "ib-controller",  # https://gist.github.com/r-rmcgibbo/9ba49cfe04e62d0e6d9d6ae3db3d6bd6

    # `git grep` in nixpkgs for usages of requireFile
]


def skip_package(drv_path: Optional[str], attr: Optional[Attr]) -> bool:
    if drv_path is None:
        return True

    for s in SKIP_DRV_NAMES:
        if f"-{s}-" in drv_path:
            return True

    if attr is not None:
        for p in SKIP_ATTR_NAMES:
            if fnmatch(attr.name, p):
                return True

    return False


def get_reduced_build_graph(
    attrs: List[Attr], modified_files: List[str]
) -> Tuple[nx.DiGraph, nx.DiGraph]:
    def buildable(attr) -> bool:
        if attr.drv_path is None:
            return False
        if attr.broken or attr.blacklisted:
            return False
        if skip_package(attr.drv_path, attr):
            return False
        return True

    buildable_drv_paths: List[str] = [a.drv_path for a in attrs if buildable(a)]  # type: ignore

    # Above about 2**13, we start hitting 'Argument list too long'
    # in build_dry. 4096 drvs should be enough for anyone. Note that because
    # of they way we run dry-runs and expand the build set to get the full
    # closure of everything we're building, it's kind of impossible for this
    # random selection to remove anything interesting. Everything that gets
    # removed and doesn't simply get added back in must have been a leaf
    # node or an entire subtree.
    N_MAX_ATTRS = 2 ** 12
    if len(buildable_drv_paths) > N_MAX_ATTRS:
        log.error(
            f"Input requested us to consider N={len(buildable_drv_paths)} attrs. "
            f"This is too many, so I'm randomly subselecting N={N_MAX_ATTRS}"
        )
        rng = random.Random(0)
        if len(modified_files) > 0:
            directly_modified_drv_paths: List[str] = [
                str(a.drv_path)
                for a in attrs
                if buildable(a) and a.filename() in modified_files
            ]
        else:
            directly_modified_drv_paths = []

        buildable_drv_paths = directly_modified_drv_paths + rng.sample(
            buildable_drv_paths, k=N_MAX_ATTRS - len(directly_modified_drv_paths)
        )

    #
    # (1) Figure out which drvs need to be built and which need to be fetched
    # in order to build the closure of all of the buildable_drv_paths.
    #
    to_build, to_fetch = build_dry(buildable_drv_paths)

    #
    # (2) We want to be a little idempotent. It makes debugging complicated if we
    # totally ignore stuff we've already built, because then we won't correctly
    # identify build roots and so on. So include in the set of 'things to build'
    # the set of things requested. This is not the set of all of the things we'd
    # need to build to build all the requested drv -- i.e. it includes the
    # intermediate dependencies and does not include things we can fetch from
    # the cache.
    to_build.update(buildable_drv_paths)

    #
    # (3) Then, we compute the dependency graph so we know which builds
    # require each other. This lets us make correct decisions about a
    # coherent subset we can build.
    g = get_build_graph(to_build)

    #
    # (4) Now, remove the stuff that we can just fetch from the build
    # graph.
    g = g.subgraph(to_build)

    # Note: previously I also was restricting to a single connected component
    # here, but that caused spacevim to be skipped here and I removed it
    # https://github.com/NixOS/nixpkgs/pull/115818

    #
    # Identify the "roots" of this build graph -- the drvs that were changed
    # in the PR or are otherwise part of the longest dependency chain. They're
    # the ones that are actually directly modified by the PR, so they're the most
    # like to be broken. Keep these directly modified attrs, keep drvs
    # directly depend on the modified attrs (shortest path length=1) or depend
    # via one intermediate "hop" (shortest path length=2). Remove stuff that's
    # more remote.
    g, closure = remove_very_indirect_dependencies(g, attrs, modified_files)

    return g, closure


def remove_very_indirect_dependencies(
    g: nx.DiGraph, attrs: List[Attr], modified_files: List[str]
) -> nx.DiGraph:
    """This is fairly subtle, and it requires a bit of tuning and
    just looking at the results on multiple PRs to see if it's
    reasonable.

    It's supposed to do a few things:

    1. `g` is the build graph. it's a DAG. each edge points from an
       more leaf-like package to a more core package that the leaf-like
       package depends on. For example, there's an edge from a numpy
       derivation to a python derivation.
    2. the graph `g` is over .drvs. many of these .drvs correspond to named
       attributes in nixpkgs, like python38Packages.numpy. Many of them do
       not. .drvs that are not attrs include `src`s and other stuff that's
       not addressable as a nixpkgs derivation. generally this is stuff that
       users do not care about.
    3. for each attr, with some execptions, we have the position filename and
       line number where it was created. We also know the set of modified files
       in this PR. From that, we can form a reasonable _guess_ of which attrs
       were actually _directly modified_ in the pr. the other attrs are ones that
       are downstream from the modification.
    4. The identification in (3) is both under- and over- inclusive. it's over-
       inclusive because we only look for a match at the level of the filename, so
       if a file contains many attrs then all of them will be flaged. it's under-
       inclusive because the PR may have changed a file that's used in attrs, but is
       not itself where any attr is declared. for example if someone changes a shell
       hook function and only edits a `.sh` file.
    5. so, because of (3) and (4), we have two schemes for identifying the "root"
       attrs that were actually changed by the PR. the first is simply which attrs
       are in files that were changed. the second is more graph theoretical. we look
       for the set of all *longest paths* in the DAG of drvs to be built, measuring
       the length of the path by the number of edges that include at least 1 attr
       in the edge. so edges that purely include non-attr .drvs don't count. and then
       we look at the deepest attr in each of these longest paths. these seem to capture
       the roots of the DAG pretty well.
    6. then, armed with these roots, we look for all nodes that are within 2 hops of
       a root, and throw away everything else. so we keep the roots themselves. we keep
       packages that directly link against these roots. and we keep edges that directly
       link against the packages that link to the roots. dependencies that are futher
       away than that get discarded.
    7. for each of these packages that are going to be retained, we record the number
       of ancestors they have. that is, how many attrs in the build graph depend directly
       or indirectly on them. note that this includes attrs in the *full graph* -- do
       this calculation before discarding the set that need to be discarded as described
       in (6). the purpose of this calculation is so that we can assign a relative
       importance to every package that we're keeping. things that are depended on by more
       packages are more important.

    So finally, the result is a new graph
    """

    non_autogenerated_modified_files = {
        m for m in modified_files if m not in BIG_AUTOGENERATED_FILES
    }

    #
    # Determine which attrs were modified directly by the git commit, rather than modified
    # indirectly because their inputs changed
    #
    drv_to_attr = {
        a.drv_path: a
        for a in attrs
        if a.drv_path is not None and a.position is not None
    }

    # Record if an edge connects to Attrs. There are some really long un-interesting
    # paths that relate to multi-lib or cross-compilation or something.
    for e in g.edges():
        g.edges[e]["is_attr"] = int(e[0] in drv_to_attr or e[1] in drv_to_attr)

    build_roots = {
        n: drv_to_attr[n].name
        for n in g.nodes
        if (
            n in drv_to_attr
            and drv_to_attr[n].filename() in non_autogenerated_modified_files
        )
    }
    log.info("Directly modified attrs", roots=json.dumps(sorted(build_roots.values())))

    #
    # Sometimes the scheme above might give no build roots, if for example
    # the only file edited was a hook or something and not where the attrs are
    # named. We want to have at least 1 build root, so if this happens lets
    # find the longest path in the dag counting only attrs, so like attr a depends
    # on b depends on c depends on d, and then let's call attr d the build root.
    #
    longest_path_build_roots = {}
    for long_path in dag_longest_paths(g, weight="is_attr"):
        *_, long_path_end = (n for n in long_path if n in drv_to_attr)
        long_path_end_name = drv_to_attr[long_path_end].name
        # log.info("Longest path", name=long_path_end_name)
        build_roots[long_path_end] = long_path_end_name
        longest_path_build_roots[long_path_end] = long_path_end_name

    log.info(
        "Longest-path build roots", roots=sorted(longest_path_build_roots.values())
    )
    log.info('Consensus build "roots"', roots=sorted(build_roots.values()))
    assert len(build_roots) > 0 or len(g) == 0

    path_length_counts: Counter_t[int] = Counter()
    to_keep = set()
    for root in build_roots:
        g.nodes[root]["is_root"] = True
        for node, path in nx.single_target_shortest_path(g, root, 2).items():
            # If node == root, len(path) == 1. Keep that, of course
            # If node is a direct dependency of root, len(path) == 2. Keep that too.
            # If node ia a 1-step indirect dependency of root, len(path) == 3. Keep that.
            # Otherwise, throw away.
            to_keep.add(node)
            path_length_counts[len(path) - 1] += 1

    #
    # Compute the closure of everything in to_keep, because we've gotta keep
    # those too.
    before_ancestor = time.time()
    transitive_closure = nx.transitive_closure_dag(g)
    if time.time() - before_ancestor > 1:
        log.info(
            f"Computing transitive closure of build graph: {time.time() - before_ancestor:.2f} sec"
        )

    #
    # Since the changed attrs were not necesarily actually roots of the build graph
    # (this happens if we're on staging and some unchanged dependencies of the changed
    # attrs actually haven't been built yet), we need to keep them in the graph. this
    # confused be at first, but if we don't keep these things in the build graph,
    # then the precedence-constrained knapsack problem won't be working with the right
    # information, because it won't know that these packages need to be built in order
    # to build the stuff we actually care about. if the "build roots" are really roots,
    # then these should be empty.
    #
    to_keep_closure = set()
    for k in to_keep:
        to_keep_closure.update(transitive_closure.succ[k])
    to_keep.update(to_keep_closure)

    if len(g) < 500:
        # Ehhhh, just keep everything if the number of nodes is small enough
        to_keep.update(g.nodes())

    for n in to_keep:
        n_ancestors = sum(nn in drv_to_attr for nn in transitive_closure.pred[n])
        g.nodes[n]["n_ancestors"] = n_ancestors

    log.info(
        "Removing remote dependencies",
        kept=len(to_keep),
        removed=g.number_of_nodes() - len(to_keep),
        total=g.number_of_nodes(),
    )

    if DEBUG:
        print("To remove")
        import IPython

        IPython.embed()

    return g.subgraph(to_keep), transitive_closure.subgraph(to_keep)


def execute(
    attrs: List[Attr], modified_files: List[str]
) -> Tuple[List[Attr], timedelta]:

    build_graph, closure = get_reduced_build_graph(attrs, modified_files)
    drv_path_to_attr = {a.drv_path: a for a in attrs if a.drv_path is not None}
    build_times = get_estimated_build_times(list(build_graph.nodes))

    profits = []
    costs = []
    drv_paths = []

    for n in build_graph.nodes():
        drv_paths.append(n)
        attr = drv_path_to_attr.get(n)
        cost = float(build_times[n])

        if cost > MAX_SINGLE_ATTR_BUILD_TIME.total_seconds():
            profit = 0.0
            cost = MAX_BUILD_TIME.total_seconds()

        if skip_package(n, attr):
            profits.append(0.0)
            costs.append(MAX_BUILD_TIME.total_seconds())
        elif attr is None:
            profits.append(-1e-5)
            costs.append(cost)
        else:
            # add 1 for self
            profit = 1 + build_graph.nodes[n]["n_ancestors"]
            if build_graph.nodes[n].get("is_root"):
                # Extra profit for building roots, since they were
                # the ones actually modified in the PR
                profit += 1000
            profits.append(profit)
            costs.append(cost)

    drv_ids = {n: i for i, n in enumerate(drv_paths)}
    edges = [(drv_ids[e[0]], drv_ids[e[1]]) for e in build_graph.edges]

    solution, status = precedenceConstrainedKnapsack(
        profit=profits,
        weight=costs,
        edges=edges,
        maxWeight=MAX_BUILD_TIME.total_seconds(),
        maxSeconds=0,
        numThreads=0,
        logLevel=0,
        allowableFractionGap=0,
        lpRelax=(len(profits) > 500),
    )

    # Mark every input attr as skipped
    for attr in attrs:
        if not (attr.broken or attr.blacklisted):
            attr.skipped = True

    # And then specifically un-skip the ones in the solution graph
    drvs_to_build = []
    for drv, x, profit, cost in zip(drv_paths, solution, profits, costs):
        if drv in drv_path_to_attr:
            attr = drv_path_to_attr[drv]
            attr.skipped = x == 0
        if x > 0:
            drvs_to_build.append(drv)

    log.info(
        f"{len(drvs_to_build)} drvs to build",
        names=sorted([n.split("-", 1)[1] for n in drvs_to_build]),
    )
    if len(drvs_to_build) < 25:
        time_to_build_roots = sum(
            build_times[n]
            for n in {
                n
                for root in build_graph
                if build_graph.nodes[root].get("is_root")
                for n in closure.succ[root]
            }
        )
        log.info(f"FYI: time to build all roots would be {time_to_build_roots:.0f} sec")
        log.info(f"FYI: time to build all drvs would be {sum(build_times.values()):.0f} sec")

    def verify_graph_correctness():
        final_to_build, final_to_fetch = build_dry(drvs_to_build)
        if final_to_build | set(drvs_to_build) != set(drvs_to_build):
            log.error(
                f"Some kind of error. We wanted to build {len(drvs_to_build)}, but we seem to be ending up with {len(final_to_build | set(drvs_to_build))}"
            )
            for x in final_to_build | set(drvs_to_build):
                if x not in drvs_to_build:
                    log.error("This seems extra", drv=x)

    if DEBUG:
        print("Line 446")
        import IPython

        IPython.embed()

    verify_graph_correctness()
    total_cost = sum(x * y for x, y in zip(solution, costs))
    return attrs, timedelta(seconds=total_cost)


def main():
    """This hook gets called by nixpkgs-review before posting
    comment to github. The comment only goes through if the
    exit status is 0.
    """
    gh = GithubClient(os.environ.get("GITHUB_TOKEN"))
    configure_logging(stderr_only=True)

    p = ArgumentParser()
    p.add_argument("--json")
    args = p.parse_args()
    if args.json is not None:
        data_in = json.load(open(args.json))
        assert DEBUG
    else:
        # Normal path
        data_in = json.load(sys.stdin)

    if DEBUG:
        with tempfile.NamedTemporaryFile(
            delete=False, prefix="nixbot-backend-pre-build-filter_", mode="w"
        ) as f:
            json.dump(data_in, f)
            log.info(f"Saved input data to {f.name}")

    # Determine which files were patched so that we can post nixpkgs-hammer
    # suggestions only for drvs that are defined in files touched by this PR.
    if "NIXPKGS_REVIEW_PR" in os.environ and isint(os.environ["NIXPKGS_REVIEW_PR"]):
        patchset = gh.load_patchset(int(os.environ["NIXPKGS_REVIEW_PR"]))
        modified_files = determine_modified_files(patchset)
    else:
        modified_files = []

    # Parse from stdin
    attrs = [Attr(**kwargs) for kwargs in data_in["attrs"]]

    result, total_cost = execute(attrs, modified_files)
    log.info(
        f"{sum(a.skipped is True for a in result)} attrs skipped by knapsack",
        skipped=[a.name for a in result if a.skipped][:10],
    )
    log.info(
        f"{sum(not (a.skipped or a.broken or a.blacklisted) for a in result)} attrs to build",
    )
    log.info(
        " ".join(sorted([a.name for a in result if not (a.skipped or a.broken or a.blacklisted)])),
    )
    log.info(
        f"Estimated build time: {total_cost.total_seconds():.0f} "
        f"sec ({naturaldelta(total_cost)}) "
        f"(out of limit {MAX_BUILD_TIME.total_seconds()} sec)"
    )

    if DEBUG:
        exit(1)

    # Report to stdout
    json.dump([a.to_dict() for a in result], fp=sys.stdout)
